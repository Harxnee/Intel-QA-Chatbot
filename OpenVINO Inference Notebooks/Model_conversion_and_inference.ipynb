{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b25984b-2979-42d0-96e7-57c885ac7dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b7b619-f6ea-4d77-83a1-8d335eb8a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the model id\n",
    "model_id = \"OjasPatil/intel-llama2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6f1327-b587-4647-890e-dce69d7be43b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1476fc1e3ac34361aed2c95cb863bae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't determine type of OV quantization config. Please specify explicitly whether you intend to run weight-only quantization or not with `weight_only` parameter. Creating an instance of OVWeightQuantizationConfig.\n",
      "The model weights will be quantized to int8.\n",
      "Using framework PyTorch: 2.3.0+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/data/venv/openvino_notebooks/openvino_2024.1.0_python3.10/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "/data/venv/openvino_notebooks/openvino_2024.1.0_python3.10/.venv/lib/python3.10/site-packages/optimum/exporters/openvino/model_patcher.py:323: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 100% (226 / 226)            │ 100% (226 / 226)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ddebde70764742aecb39806032fbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the Hugging Face Hub with OpenVINO optimization\n",
    "model = OVModelForCausalLM.from_pretrained(model_id, \n",
    "                                          export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a150cd-264d-4e69-9468-f9d8d06b076c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer associated with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "196d767d-ac40-4db7-8ac9-d3d5129c04fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel OpenVINO is an open-source software development kit for deploying and tracking deep learning models across various applications, environments, and platforms. It enables real-time communication between models and applications like PyTorch, TensorFlow,\n"
     ]
    }
   ],
   "source": [
    "# Define the first user prompt\n",
    "user_prompt = \"What is Intel OpenVINO?\"\n",
    "prompt = f\"<s>[INST] {user_prompt} [/INST]\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# Decode the output tokens to a string, skip the prompt and initial format\n",
    "result = tokenizer.decode(outputs[0])[len(prompt)+5:]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f1087dd-7acb-440b-9c12-88bc196094e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Intel toolkits do not qualify for Priority Support. However, you can still ask questions in the community forum [https://community.intel.com/t5/Software-Products/ct-p/software-products\n"
     ]
    }
   ],
   "source": [
    "# Define the second user prompt\n",
    "user_prompt = \"Do you offer support for free Intel toolkits?\"\n",
    "prompt = f\"<s>[INST] {user_prompt} [/INST]\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "\n",
    "# Decode the output tokens to a string, skip the prompt and initial format\n",
    "result = tokenizer.decode(outputs[0])[len(prompt)+5:]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b8d4f7f-d350-4e63-9b34-8b87eae1442c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo, listen up, it's time to flow, About Intel OpenVINO, the software that makes my models go.  It's open source, you see, and that's a win for me.  Permission\n"
     ]
    }
   ],
   "source": [
    "# Define the third user prompt with a specific instruction to answer in rap language\n",
    "user_prompt = \"What is Intel OpenVINO?\"\n",
    "prompt = f\"<s>[INST]<<SYS>>Answer the question in rap language</SYS>> {user_prompt} [/INST]\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "result = tokenizer.decode(outputs[0])[len(prompt)+5:]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2e159c3-d546-419f-85fc-d69a623bddcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model locally\n",
    "model.save_pretrained(\"intel_llama2_7b_ov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3303786d-272a-4f6a-ada0-4c5b90911da3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118660fa451a4bdb9de3c80c582ec5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aee67f63-8f3e-405c-b2f1-4bc1223b5e72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b514ef8cf61849949a0063feeaca3984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.bin:   0%|          | 0.00/6.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Push the model to the Hugging Face Hub\n",
    "model.push_to_hub(repository_id = \"OjasPatil/intel-llama2-7b-ov\", save_directory=\"intel_llama2_7b_ov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa29e366-9d82-45c9-be61-080040ef9bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4eb6f1fedf4d2995cdcdeb1d81eea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4b393ece34423da9d764dcacf70a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/OjasPatil/intel-llama2-7b-ov/commit/33551787372f118327dab631db387499fdacccdb', commit_message='Upload tokenizer', commit_description='', oid='33551787372f118327dab631db387499fdacccdb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the tokenizer to the Hugging Face Hub\n",
    "tokenizer.push_to_hub(\"OjasPatil/intel-llama2-7b-ov\",check_pr=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
