{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfb5aa6-f8fe-4b70-8d32-5cbcecd929af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "\n",
    "# lets load the openVINO format model from huggingface hub\n",
    "# link to hf repo - https://huggingface.co/OjasPatil/intel-llama2-7b-ov\n",
    "model_name = \"OjasPatil/intel-llama2-7b-ov\"\n",
    "\n",
    "base_model = OVModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d457fac0-5f8e-404d-904a-d1e50d8f1921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, free Intel toolkits do not qualify for Priority Support. However, you can still ask questions in the community forum [https://community.intel.com/t5/Software-Products/ct-p/software\n"
     ]
    }
   ],
   "source": [
    "message = \"Do you offer support for free Intel toolkits?\"\n",
    "prompt = f\"[INST] {message} [/INST]\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = base_model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt+\" \", \"\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a43e1f6-f45a-414f-bbe5-7537d65abd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.28.3, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "def respond(\n",
    "    message,\n",
    "    history: list[tuple[str, str]],\n",
    "    system_message,\n",
    "    max_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "):\n",
    "    # prompt = f\"<s>[INST] {system_message} [/INST] {message}\"\n",
    "    prompt = f\"[INST]{message}[/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = base_model.generate(**inputs, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\")\n",
    "    \n",
    "    yield response\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    respond,\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.95,\n",
    "            step=0.05,\n",
    "            label=\"Top-p (nucleus sampling)\",\n",
    "        ),\n",
    "    ],\n",
    "    title=\"Intel Virtual Assistant Chatbot\"\n",
    ")\n",
    "\n",
    "# demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6773f9aa-ff89-496d-b090-78379a8546e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr\n",
    "\n",
    "from qarpo import get_gradio_setup\n",
    "\n",
    "try:\n",
    "    gradio_interface_args, access_details = get_gradio_setup()\n",
    "    demo.queue(max_size=2).launch(**gradio_interface_args)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0386b78b-6178-4f60-9dfa-eddc081d2825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The Gradio App is ready. Use the following URL to access it:\n",
      "        \n",
      "        URL: https://notebooks.one-edge.intel.com/hub/user-redirect/proxy/8000/\n",
      "\n",
      "\n",
      "    Notes: The Gradio application is configured with default secure parameters.\n",
      "           Modifying or disabling such configuration might result in unexpected\n",
      "           data exposure. For more information about Gradio Security, see:\n",
      "\n",
      "           https://www.gradio.app/guides/sharing-your-app#security-and-file-access\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(access_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a76bbf-0521-48ef-9cb5-f0fadc78f3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 8000\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9405a337-2023-4497-ac69-20cbd5f1d637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81430ac2a63448d8afd06736c837211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "\n",
    "# loads the non-finetuned original llama2-7b model\n",
    "model_name_1 = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "base_model_1 = AutoModelForCausalLM.from_pretrained(model_name_1)\n",
    "\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(model_name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cc7302-642f-4e26-9309-08faa8c4d2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.28.3, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "def respond(\n",
    "    message,\n",
    "    history: list[tuple[str, str]],\n",
    "    system_message,\n",
    "    max_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "):\n",
    "    # prompt = f\"<s>[INST] {system_message} [/INST] {message}\"\n",
    "    prompt = f\"[INST]{message}[/INST]\"\n",
    "    inputs = tokenizer_1(prompt, return_tensors=\"pt\")\n",
    "    outputs = base_model_1.generate(**inputs, max_new_tokens=100)\n",
    "    response = tokenizer_1.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\")\n",
    "    \n",
    "    yield response\n",
    "\n",
    "demo_og = gr.ChatInterface(\n",
    "    respond,\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.95,\n",
    "            step=0.05,\n",
    "            label=\"Top-p (nucleus sampling)\",\n",
    "        ),\n",
    "    ],\n",
    "    title=\"Llama2 7b Model\"\n",
    ")\n",
    "\n",
    "# demo_og.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "748207f3-0687-496b-9813-76fa3b8ac864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr\n",
    "\n",
    "from qarpo import get_gradio_setup\n",
    "\n",
    "try:\n",
    "    gradio_interface_args, access_details = get_gradio_setup()\n",
    "    demo_og.queue(max_size=2).launch(**gradio_interface_args)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30a7d626-44cc-48b1-82ce-46207e3506eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The Gradio App is ready. Use the following URL to access it:\n",
      "        \n",
      "        URL: https://notebooks.one-edge.intel.com/hub/user-redirect/proxy/8001/\n",
      "\n",
      "\n",
      "    Notes: The Gradio application is configured with default secure parameters.\n",
      "           Modifying or disabling such configuration might result in unexpected\n",
      "           data exposure. For more information about Gradio Security, see:\n",
      "\n",
      "           https://www.gradio.app/guides/sharing-your-app#security-and-file-access\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(access_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ccc2be8-7752-4d8b-bb80-b3f293f5492b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 8001\n"
     ]
    }
   ],
   "source": [
    "demo_og.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e3efa-2556-447f-a553-5bb76e3d52f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (OpenVINO Notebooks 2024.1.0)",
   "language": "python",
   "name": "openvino_notebooks_2024.1.0_python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
